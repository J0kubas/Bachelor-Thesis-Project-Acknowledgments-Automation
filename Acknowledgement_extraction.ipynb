{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXXvkSPqVMjy",
        "outputId": "7246c976-1338-4098-efbc-f10a94a1c2ba"
      },
      "outputs": [],
      "source": [
        "!pip install -U spacy[transformers]\n",
        "!python -m spacy download en_core_web_trf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOfx6IcnVUrI",
        "outputId": "3f68ebbc-ab6c-4d5f-a944-0b9bbde6301f"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "spacy.require_gpu()\n",
        "print(\"Using GPU:\", spacy.prefer_gpu())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3u_CLfGLW5ez",
        "outputId": "d027bdcc-5de9-4410-f1fb-3912c84da948"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUsKuniY-5qK",
        "outputId": "e845422a-6b7b-450d-c405-829b8f8bf39e"
      },
      "outputs": [],
      "source": [
        "#secondary part\n",
        "import json\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_trf\")\n",
        "\n",
        "input_json_path = \"/content/drive/MyDrive/ack_clen_newline_final.json\"\n",
        "output_json_path = \"/content/drive/MyDrive/new_ner_output.json\"\n",
        "\n",
        "with open(input_json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "for entry_id, entry in data.items():\n",
        "    ack_text = entry.get(\"AckSection\", \"\").strip()\n",
        "\n",
        "    if ack_text:\n",
        "        doc = nlp(ack_text)\n",
        "        orgs = list(set(ent.text.strip() for ent in doc.ents if ent.label_ == \"ORG\"))\n",
        "        persons = list(set(ent.text.strip() for ent in doc.ents if ent.label_ == \"PERSON\"))\n",
        "\n",
        "        entry[\"ner\"] = {\n",
        "            \"orgs\": orgs,\n",
        "            \"person\": persons\n",
        "        }\n",
        "\n",
        "    else:\n",
        "        entry[\"ner\"] = {\n",
        "            \"orgs\": [],\n",
        "            \"person\": []\n",
        "        }\n",
        "\n",
        "    if \"entity_roles\" in entry:\n",
        "        del entry[\"entity_roles\"]\n",
        "\n",
        "with open(output_json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(\"done\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P2P4cvy0YHtM"
      },
      "outputs": [],
      "source": [
        "folder_path = \"/content/drive/MyDrive/texts\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aAuZ_GbAVVAY"
      },
      "outputs": [],
      "source": [
        "#original ackextract\n",
        "import re\n",
        "import json\n",
        "import os\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_trf\")\n",
        "\n",
        "ACK_START_PATTERNS = [\n",
        "    r'^acknowledg(e)?ments?\\.$',\n",
        "    r'^acknowledg(e)?ments?[\\.:]?\\s*$',\n",
        "    r'^acknowledg(e)?ments?[\\.:]?\\s+',\n",
        "    r'^\\**acknowledg(e)?ments?\\**[\\.:]?\\s*$',\n",
        "    r'^section\\s*\\d+[:\\.\\)]?\\s*acknowledg(e)?ments?$',\n",
        "    r'^\\d+\\.\\s*acknowledg(e)?ments?$',\n",
        "    r'^acknowledg(e)?ments?\\s*\\n',\n",
        "\n",
        "]\n",
        "SECTION_STOP_WORDS = [\n",
        "    \"references\", \"bibliography\", \"appendix\", \"abstract\"\n",
        "]\n",
        "\n",
        "def is_acknowledgment_start(sent):\n",
        "    stripped = sent.text.strip().lower()\n",
        "    return any(re.match(p, stripped) for p in ACK_START_PATTERNS)\n",
        "\n",
        "def is_section_end(sent):\n",
        "    return sent.text.strip().lower() in SECTION_STOP_WORDS\n",
        "\n",
        "def extract_acknowledgments(text):\n",
        "    text = text.replace('\\n', ' ')\n",
        "    doc = nlp(text)\n",
        "    ack_sentences = []\n",
        "    collecting = False\n",
        "\n",
        "\n",
        "    for sent in doc.sents:\n",
        "        stripped = sent.text.strip()\n",
        "        stripped_lower = stripped.lower()\n",
        "\n",
        "        if is_acknowledgment_start(sent) or stripped_lower.startswith(\"acknowledg\"):\n",
        "            collecting = True\n",
        "            ack_sentences.append(sent)\n",
        "            continue\n",
        "\n",
        "\n",
        "        if collecting:\n",
        "            if is_section_end(sent):\n",
        "                break\n",
        "            if re.match(r'^[1-9]\\d*\\.', stripped):\n",
        "                break\n",
        "            if len(stripped.split()) < 3:\n",
        "                break\n",
        "            ack_sentences.append(sent)\n",
        "\n",
        "    if not ack_sentences:\n",
        "        return None, [], []\n",
        "\n",
        "\n",
        "    ack_text = ' '.join(sent.text for sent in ack_sentences)\n",
        "\n",
        "\n",
        "    ack_doc = nlp(ack_text)\n",
        "\n",
        "    orgs = list(set(ent.text.strip() for ent in ack_doc.ents if ent.label_ == \"ORG\"))\n",
        "    persons = list(set(ent.text.strip() for ent in ack_doc.ents if ent.label_ == \"PERSON\"))\n",
        "\n",
        "    return ack_text.strip(), orgs, persons\n",
        "\n",
        "def extract_ack_from_tail(text):\n",
        "    text = re.sub(r'[ \\t]+', ' ', text)\n",
        "\n",
        "    ref_match = re.search(r'\\n\\n(?:references?|bibliography)\\b.*', text, flags=re.IGNORECASE | re.DOTALL)\n",
        "    if not ref_match:\n",
        "        return None\n",
        "\n",
        "    ref_start = ref_match.start()\n",
        "\n",
        "    ack_match = None\n",
        "    for match in re.finditer(r'\\n\\nacknowledg(e)?ments?\\b.*', text[:ref_start], flags=re.IGNORECASE | re.DOTALL):\n",
        "        ack_match = match\n",
        "\n",
        "    if not ack_match:\n",
        "        return None\n",
        "\n",
        "    ack_start = ack_match.start()\n",
        "    ack_text = text[ack_start:ref_start]\n",
        "\n",
        "    cleaned_ack_text = ' '.join(ack_text.split())\n",
        "\n",
        "    return cleaned_ack_text\n",
        "\n",
        "def extract_ner_org_affiliations(text_block):\n",
        "    doc = nlp(text_block)\n",
        "    return list(set(ent.text.strip() for ent in doc.ents if ent.label_ == \"ORG\"))\n",
        "\n",
        "def extract_ner_prs_affiliations(text_block):\n",
        "    doc = nlp(text_block)\n",
        "    return list(set(ent.text.strip() for ent in doc.ents if ent.label_ == \"PERSON\"))\n",
        "\n",
        "def process_file(filepath):\n",
        "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
        "        raw_text = f.read()\n",
        "\n",
        "    ack_section, orgs, persons = extract_acknowledgments(raw_text)\n",
        "\n",
        "    if ack_section is None:\n",
        "        ack_section = extract_ack_from_tail(raw_text)\n",
        "        if ack_section is not None:\n",
        "          orgs = extract_ner_org_affiliations(ack_section)\n",
        "          persons = extract_ner_prs_affiliations(ack_section)\n",
        "        else:\n",
        "          orgs = []\n",
        "          persons = []\n",
        "\n",
        "    return {\n",
        "        \"ner\": {\n",
        "            \"orgs\": orgs,\n",
        "            \"person\": persons\n",
        "        },\n",
        "        \"AckSection\": ack_section\n",
        "    }\n",
        "\n",
        "\n",
        "def process_folder(folder_path):\n",
        "    results = {}\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith(\".txt\"):\n",
        "            file_path = os.path.join(folder_path, filename)\n",
        "            results[filename] = process_file(file_path)\n",
        "    return results\n",
        "\n",
        "\n",
        "output = process_folder(folder_path)\n",
        "\n",
        "\n",
        "with open(\"/content/drive/MyDrive/newAck.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(output, f, indent=2, ensure_ascii=False)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
